# ECHO: Enhanced Communication for Hearing Impaired in Online Podcasts

ECHO is a groundbreaking system designed to make online podcasts accessible for hearing-impaired individuals. This project identifies and visualizes the active speaker ğŸ—£ï¸, synchronizes subtitles ğŸ“, and even provides Hindi translations ğŸ‡®ğŸ‡³ for enhanced inclusivity.  

ğŸ–‹ï¸ **Presented at:**  
**ACM 8th International Conference on Data Science and Management of Data (CODS COMAD)**  
ğŸ“… **Date:** December 18â€“21, 2024  
ğŸ“ **Location:** IIT Jodhpur, India  

---

## âœ¨ Features
- ğŸ—£ï¸ **Speaker Identification**: Detects the active speaker in a video using lip movement and audio analysis.  
- ğŸ“ **Subtitle Synchronization**: Automatically generates synchronized subtitles.  
- ğŸ¥ **Multimodal Integration**: Combines **video** (face and lip movement detection) with **audio** (speech transcription and speaker classification).  
- ğŸŒ **Hindi Translations**: Automatically translates English subtitles into Hindi for better accessibility.  
- ğŸ“‚ **Benchmark Dataset**: Includes 500 annotated videos, diverse in accents and genres, tailored for speaker identification.  

---

## ğŸ› ï¸ Architecture Overview
ECHO integrates state-of-the-art models for seamless functionality:  
### ğŸ“¹ **Video Processing**  
- **Face Detection**: [MTCNN](https://arxiv.org/abs/1604.02878) detects faces with high accuracy.  
- **Lip Movement Detection**: [LipNet](https://arxiv.org/abs/1611.01599) analyzes lip movements for speaker identification.  

### ğŸ™ï¸ **Audio Processing**  
- **Speech Transcription**: Powered by OpenAIâ€™s [Whisper](https://github.com/openai/whisper).  
- **Speaker Embeddings**: Extracted with [Wav2Vec 2.0](https://arxiv.org/abs/2006.11477).  
- **Clustering**: Groups speaker data for robust classification.  

### ğŸ”— **Integration**  
- Syncs video and audio streams in real-time for seamless output.  
- Bounding boxes and color-coded speakers make the experience intuitive.

![Model Overview](images/block_diagram.png)  
*Figure: Block diagram of the ECHO architecture.*

---

## ğŸ“Š Evaluation
The system excels across multiple benchmarks:  
- ğŸ† **Word Error Rate (WER)**: 5.3% â€“ delivering accurate transcriptions.  
- ğŸ† **Speaker Error Rate (SER)**: 9.2% â€“ ensuring precise speaker classification.  

### ğŸ” Ablation Studies
- Integration of **audio and video models** improves accuracy by 8%.  
- Incorporating **lip detection** reduces synchronization errors by 4%.

---

## ğŸ“‚ Dataset
The dataset includes:  
- ğŸ¥ 500 conversation videos, annotated with English subtitles in `.srt` format.  
- ğŸŒ Designed for diverse accents and genres.  
- ğŸ“¥ **[Download a sample of the dataset here](https://drive.google.com/drive/folders/1-5IJ-gzepjWvkoEkowTQfArjBg2QZD_A?usp=drive_link)**.  

---

## ğŸš€ Usage
### âš™ï¸ Prerequisites  
- Python 3.8 or higher  
- Required libraries: `torch`, `transformers`, `opencv-python`, `librosa`, `scikit-learn`

### ğŸ”§ Installation
1. Clone the repository:  
   ```bash
   git clone https://github.com/your-repo/echo.git
   cd echo
2. Install dependencies:
    ```bash
    pip install -r requirements.txt
### â–¶ï¸ Running the Model
1. Place input videos in the data/input directory.
2. Process videos
    ```bash
    python main.py --input data/input --output data/output
3. Output videos with subtitles will be saved in the data/output directory. ğŸ‰
---
## ğŸ… Results
The system supports multiple video genres:

- ğŸ™ï¸ Talk shows: WER 9.0%, SER 10.3%
- ğŸ¤ Interviews: WER 8.5%, SER 9.5%
- ğŸ—³ï¸ Political debates: WER 5.1%, SER 9.0%
---
## ğŸ“œ Citation
If you use this work in your research, please cite:
```
@inproceedings{godhala2024echo,
  title={ECHO: Enhanced Communication for Hearing Impaired in Online Podcasts},
  author={Gouthami Godhala, Vijayasree Asam, Samriddha Sanyal},
  booktitle={Proceedings of the 8th International Conference on Data Science and Management of Data (CODS-COMAD)},
  year={2024},
  doi={10.1145/3703323.3703345}
}
```
## ğŸ™ Acknowledgments
This project was supported by the **Centre for Interdisciplinary Artificial Intelligence (CAI), FLAME University.**

## ğŸ‘¥ Contributors  
- **Gouthami Godhala** ([GitHub Profile](https://github.com/gouthamireddy2507))  
- **Vijayasree Asam** ([GitHub Profile](https://github.com/vijayasree1284))  
- **Samriddha Sanyal**
